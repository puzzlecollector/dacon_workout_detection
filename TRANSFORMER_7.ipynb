{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRANSFORMER 7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH6GTDLiedD-"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os, datetime \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *  \n",
        "from tensorflow.keras.callbacks import *\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold \n",
        "from tqdm import tqdm\n",
        "import random \n",
        "import time\n",
        "import pywt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofD6qOZQR8Y-",
        "outputId": "b54734c8-3319-40f9-cd7d-ddea687ce8a9"
      },
      "source": [
        "pip install tsaug"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tsaug\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/6e/8b1be145a32bba360c14322c3b87ad93d6227c46528d482c84eefe54094b/tsaug-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.6/dist-packages (from tsaug) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from tsaug) (1.19.5)\n",
            "Installing collected packages: tsaug\n",
            "Successfully installed tsaug-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGL7x71RR8TJ"
      },
      "source": [
        "import tsaug\n",
        "from tsaug import TimeWarp, Crop, Quantize, Drift, Reverse "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idtyqVibVGL1"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCAeyEfe0PgM"
      },
      "source": [
        "## Load data \n",
        "train_features = pd.read_csv('drive/MyDrive/movement_detection/train_features.csv')\n",
        "train_labels = pd.read_csv('drive/MyDrive/movement_detection/train_labels.csv')\n",
        "test_features = pd.read_csv('drive/MyDrive/movement_detection/test_features.csv')\n",
        "sample_submission = pd.read_csv('drive/MyDrive/movement_detection/sample_submission.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UXUMFz2B3Pq",
        "outputId": "e73600ea-ac4d-4a84-8277-7f7833a06bbe"
      },
      "source": [
        "X = tf.reshape(np.array(train_features.iloc[:,2:]),[-1, 600, 6])\n",
        "X = np.asarray(X) \n",
        "X.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3125, 600, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZCnLTIbB5xj",
        "outputId": "fe8e6145-1f65-4cd8-f01e-744c307ca063"
      },
      "source": [
        "y = train_labels['label'].values\n",
        "y.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3125,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVGWLvvUQvtG"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVqq_B3MQxQ8"
      },
      "source": [
        "batch_size = 32\n",
        "seq_len = 600 # temporary \n",
        "d_k = 256 \n",
        "d_v = 256\n",
        "n_heads = 12 \n",
        "ff_dim = 512"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D6aSnc9QyFd"
      },
      "source": [
        "class SingleAttention(Layer):\n",
        "    def __init__(self, d_k, d_v):\n",
        "        super(SingleAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.query = Dense(self.d_k, \n",
        "                       input_shape=input_shape, \n",
        "                       kernel_initializer='glorot_uniform', \n",
        "                       bias_initializer='glorot_uniform')\n",
        "    \n",
        "        self.key = Dense(self.d_k, \n",
        "                     input_shape=input_shape, \n",
        "                     kernel_initializer='glorot_uniform', \n",
        "                     bias_initializer='glorot_uniform')\n",
        "    \n",
        "        self.value = Dense(self.d_v, \n",
        "                       input_shape=input_shape, \n",
        "                       kernel_initializer='glorot_uniform', \n",
        "                       bias_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
        "        q = self.query(inputs[0])\n",
        "        k = self.key(inputs[1])\n",
        "\n",
        "        attn_weights = tf.matmul(q, k, transpose_b=True)\n",
        "        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
        "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
        "    \n",
        "        v = self.value(inputs[2])\n",
        "        attn_out = tf.matmul(attn_weights, v)\n",
        "        return attn_out    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HF7ZzPXQyDP"
      },
      "source": [
        "class MultiAttention(Layer):\n",
        "    def __init__(self, d_k, d_v, n_heads):\n",
        "        super(MultiAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.n_heads = n_heads\n",
        "        self.attn_heads = list()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        for n in range(self.n_heads):\n",
        "            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n",
        "    \n",
        "        self.linear = Dense(input_shape[0][-1], \n",
        "                        input_shape=input_shape, \n",
        "                        kernel_initializer='glorot_uniform', \n",
        "                        bias_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
        "        concat_attn = tf.concat(attn, axis=-1)\n",
        "        multi_linear = self.linear(concat_attn)\n",
        "        return multi_linear   \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzPC6gaJQyBT"
      },
      "source": [
        "class TransformerEncoder(Layer):\n",
        "    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.n_heads = n_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.attn_heads = []\n",
        "        self.dropout_rate = dropout \n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
        "        self.attn_dropout = Dropout(self.dropout_rate)\n",
        "        self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
        "\n",
        "        self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
        "        self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n",
        "        self.ff_dropout = Dropout(self.dropout_rate)\n",
        "        self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n",
        "  \n",
        "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq) \n",
        "        attn_layer = self.attn_multi(inputs) \n",
        "        attn_layer = self.attn_dropout(attn_layer)\n",
        "        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
        "        ff_layer = self.ff_conv1D_1(attn_layer)  \n",
        "        ff_layer = self.ff_conv1D_2(ff_layer)\n",
        "        ff_layer = self.ff_dropout(ff_layer)\n",
        "        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
        "        return ff_layer \n",
        "\n",
        "    def get_config(self): # Needed for saving and loading model with custom layer\n",
        "        config = super().get_config().copy()\n",
        "        config.update({'d_k': self.d_k,\n",
        "                   'd_v': self.d_v,\n",
        "                   'n_heads': self.n_heads,\n",
        "                   'ff_dim': self.ff_dim,\n",
        "                   'attn_heads': self.attn_heads,\n",
        "                   'dropout_rate': self.dropout_rate})\n",
        "        return config\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB_RU8LwQx_b"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auqFmF-OQx7M"
      },
      "source": [
        "def build_model(seq_len, features): \n",
        "  inputs = Input(shape=(seq_len,features))   \n",
        "  bn = BatchNormalization()(inputs)    \n",
        "\n",
        "  conv = Conv1D(256, 5, activation = 'relu', padding = 'same')(bn) \n",
        "  bn = BatchNormalization()(conv) \n",
        "  maxpool = MaxPooling1D()(bn) \n",
        "  conv = Conv1D(256, 3, activation = 'relu', padding = 'same')(maxpool) \n",
        "  bn = BatchNormalization()(conv) \n",
        "  features = MaxPooling1D()(bn) \n",
        "\n",
        "  pos_encoding = positional_encoding(seq_len, d_k)    \n",
        "  features *= tf.math.sqrt(tf.cast(d_k, tf.float32)) # scale  \n",
        "  features += pos_encoding[:, :features.shape[1], :] # add positional encoding \n",
        "\n",
        "  attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "  attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "  attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "\n",
        "  x = attn_layer1((features, features, features))   \n",
        "  x = attn_layer2((x, x, x)) \n",
        "  x = attn_layer3((x, x, x)) \n",
        "\n",
        "  bi_gru = Bidirectional(GRU(128, return_sequences = False))(x) \n",
        "  dropout = Dropout(0.25)(bi_gru) \n",
        "  dense = Dense(128, activation = 'relu')(dropout) \n",
        "  bn = BatchNormalization()(dense) \n",
        "  outputs = Dense(61, activation='softmax')(bn) \n",
        "  model = Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz_fT0fyQx0W",
        "outputId": "061ee8ed-2596-4fc3-c704-93e7f499cac5"
      },
      "source": [
        "model = build_model(600, 18)\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 600, 18)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 600, 18)      72          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 600, 256)     23296       batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 600, 256)     1024        conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 300, 256)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 300, 256)     196864      max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 300, 256)     1024        conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 150, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply (TFOpLambda)   (None, 150, 256)     0           max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add (TFOpLambd (None, 150, 256)     0           tf.math.multiply[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder (Transforme (None, 150, 256)     3419136     tf.__operators__.add[0][0]       \n",
            "                                                                 tf.__operators__.add[0][0]       \n",
            "                                                                 tf.__operators__.add[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder_1 (Transfor (None, 150, 256)     3419136     transformer_encoder[0][0]        \n",
            "                                                                 transformer_encoder[0][0]        \n",
            "                                                                 transformer_encoder[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder_2 (Transfor (None, 150, 256)     3419136     transformer_encoder_1[0][0]      \n",
            "                                                                 transformer_encoder_1[0][0]      \n",
            "                                                                 transformer_encoder_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 256)          296448      transformer_encoder_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 256)          0           bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          32896       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128)          512         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 61)           7869        batch_normalization_3[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 10,817,413\n",
            "Trainable params: 10,816,097\n",
            "Non-trainable params: 1,316\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16jUDPoVRRcV",
        "outputId": "637868b2-e242-47f7-b6c5-4e38c2ca02ef"
      },
      "source": [
        "kfold = StratifiedKFold(n_splits = 10, random_state = 960418, shuffle = True)\n",
        "for idx,(train_idx, val_idx) in enumerate(kfold.split(X,y)):   \n",
        "    print(\"... Validating on fold {} ...\".format(idx+1))  \n",
        "    if idx < 4: \n",
        "      continue \n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx] \n",
        "    \n",
        "    ##### augment data #####\n",
        "    print(\"... Augmenting Data ...\")\n",
        "    X_augmented = [] \n",
        "    y_augmented = [] \n",
        "\n",
        "    for i in tqdm(range(X_train.shape[0]), position = 0, leave = True): \n",
        "        for j in range(10): # add random shift \n",
        "            shifted = np.roll(X_train[i], int(random.random() * 600)) \n",
        "            X_augmented.append(shifted) \n",
        "            y_augmented.append(y_train[i]) \n",
        "        for j in range(10): # add noise \n",
        "            noised = np.random.normal(0, 1, X_train[i].shape) + X_train[i] \n",
        "            X_augmented.append(noised) \n",
        "            y_augmented.append(y_train[i])   \n",
        "    \n",
        "\n",
        "    X_cropped = Crop(random.randint(300, 580), resize = 600).augment(X_train) \n",
        "    X_warped = TimeWarp(random.randint(3,20)).augment(X_train)  \n",
        "    X_reversed = Reverse().augment(X_train) \n",
        "    X_quantized = Quantize(random.randint(10,100)).augment(X_train)\n",
        "    x_drift = Drift(max_drift=(0.1,0.5)).augment(X_train) # similar to random shift \n",
        "    x_drift2 = Drift(max_drift=(0.1,0.5)).augment(X_train)  \n",
        "    x_dropout = tsaug.Dropout(p=(0.1,0.5)).augment(X_train)\n",
        "\n",
        "    X_augmented = np.asarray(X_augmented) \n",
        "    y_augmented = np.asarray(y_augmented)    \n",
        "\n",
        "    X_train = np.concatenate([X_train, X_augmented, X_cropped, X_warped, X_reversed, X_quantized, x_drift, x_drift2, x_dropout]) \n",
        "    y_train = np.concatenate([y_train, y_augmented, y_train, y_train, y_train, y_train, y_train, y_train, y_train])  \n",
        "\n",
        "    print(\"Train shapes\")\n",
        "    print(X_train.shape, y_train.shape)  \n",
        "\n",
        "    ###### feature engineering data ##### \n",
        "    print(\"... DFFT Feature Engineering ...\")\n",
        "    X_fourier_real = [] \n",
        "    X_fourier_imag = [] \n",
        "    for i in tqdm(range(X_train.shape[0]), position = 0, leave = True):  \n",
        "        real_part = np.fft.fft(X_train[i]).real \n",
        "        imag_part = np.fft.fft(X_train[i]).imag \n",
        "        X_fourier_real.append(real_part)\n",
        "        X_fourier_imag.append(imag_part) \n",
        "    \n",
        "    X_fourier_real = np.asarray(X_fourier_real)  \n",
        "    X_fourier_imag = np.asarray(X_fourier_imag)\n",
        "    \n",
        "    X_val_fourier_real = [] \n",
        "    X_val_fourier_imag = [] \n",
        "    for i in tqdm(range(X_val.shape[0]), position = 0, leave = True):\n",
        "        real_part = np.fft.fft(X_val[i]).real \n",
        "        imag_part = np.fft.fft(X_val[i]).imag \n",
        "        X_val_fourier_real.append(real_part) \n",
        "        X_val_fourier_imag.append(imag_part)\n",
        "    \n",
        "    X_val_fourier_real = np.asarray(X_val_fourier_real) \n",
        "    X_val_fourier_imag = np.asarray(X_val_fourier_imag)\n",
        "    \n",
        "    X_train = np.concatenate([X_train, X_fourier_real, X_fourier_imag], axis = 2)  \n",
        "    X_val = np.concatenate([X_val, X_val_fourier_real, X_val_fourier_imag], axis = 2)\n",
        "\n",
        "    ##### train model #####  \n",
        "    print(\"... Building Model ...\")\n",
        "    # we have 18 features after feature engineering \n",
        "    model = build_model(600, 18) \n",
        "    print(\"... Training ...\") \n",
        "    model_path = 'drive/MyDrive/movement_detection/kfold' + str(idx+1) + '/TRANSFORMER7_epoch_{epoch:03d}_val_{val_loss:.3f}_accuracy_{val_accuracy:.3f}.h5'\n",
        "    learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', patience = 1, verbose = 1, factor = 0.8)\n",
        "    checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
        "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5) \n",
        "\n",
        "    model.fit(X_train,\n",
        "              y_train,\n",
        "              epochs = 200,\n",
        "              batch_size = 32, # hyperparameter  \n",
        "              validation_data = (X_val, y_val), \n",
        "              callbacks = [learning_rate_reduction, checkpoint, early_stopping])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... Validating on fold 1 ...\n",
            "... Validating on fold 2 ...\n",
            "... Validating on fold 3 ...\n",
            "... Validating on fold 4 ...\n",
            "... Validating on fold 5 ...\n",
            "... Augmenting Data ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2812/2812 [00:06<00:00, 441.98it/s]\n",
            "  1%|          | 779/78736 [00:00<00:10, 7783.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train shapes\n",
            "(78736, 600, 6) (78736,)\n",
            "... DFFT Feature Engineering ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78736/78736 [00:11<00:00, 6565.77it/s]\n",
            "100%|██████████| 313/313 [00:00<00:00, 8884.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "... Building Model ...\n",
            "... Training ...\n",
            "Epoch 1/200\n",
            "2461/2461 [==============================] - 953s 374ms/step - loss: 2.2794 - accuracy: 0.5093 - val_loss: 1.4439 - val_accuracy: 0.6134\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.44387, saving model to drive/MyDrive/movement_detection/kfold5/TRANSFORMER7_epoch_001_val_1.444_accuracy_0.613.h5\n",
            "Epoch 2/200\n",
            "2461/2461 [==============================] - 905s 368ms/step - loss: 1.3499 - accuracy: 0.6308 - val_loss: 1.0831 - val_accuracy: 0.7093\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.44387 to 1.08311, saving model to drive/MyDrive/movement_detection/kfold5/TRANSFORMER7_epoch_002_val_1.083_accuracy_0.709.h5\n",
            "Epoch 3/200\n",
            "2461/2461 [==============================] - 900s 366ms/step - loss: 0.9740 - accuracy: 0.7230 - val_loss: 0.8846 - val_accuracy: 0.7188\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.08311 to 0.88463, saving model to drive/MyDrive/movement_detection/kfold5/TRANSFORMER7_epoch_003_val_0.885_accuracy_0.719.h5\n",
            "Epoch 4/200\n",
            "2461/2461 [==============================] - 899s 365ms/step - loss: 0.7554 - accuracy: 0.7812 - val_loss: 0.9905 - val_accuracy: 0.7444\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.88463\n",
            "Epoch 5/200\n",
            "2461/2461 [==============================] - 896s 364ms/step - loss: 0.6330 - accuracy: 0.8165 - val_loss: 0.8626 - val_accuracy: 0.7444\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.88463 to 0.86262, saving model to drive/MyDrive/movement_detection/kfold5/TRANSFORMER7_epoch_005_val_0.863_accuracy_0.744.h5\n",
            "Epoch 6/200\n",
            "2461/2461 [==============================] - 899s 365ms/step - loss: 0.5352 - accuracy: 0.8433 - val_loss: 0.9338 - val_accuracy: 0.7636\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.86262\n",
            "Epoch 7/200\n",
            "2461/2461 [==============================] - 898s 365ms/step - loss: 0.4556 - accuracy: 0.8673 - val_loss: 0.9279 - val_accuracy: 0.7827\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.86262\n",
            "Epoch 8/200\n",
            "2461/2461 [==============================] - 892s 363ms/step - loss: 0.3842 - accuracy: 0.8857 - val_loss: 0.9029 - val_accuracy: 0.7636\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.86262\n",
            "Epoch 9/200\n",
            "2461/2461 [==============================] - 894s 363ms/step - loss: 0.3251 - accuracy: 0.9032 - val_loss: 0.9175 - val_accuracy: 0.7700\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.86262\n",
            "Epoch 10/200\n",
            "2461/2461 [==============================] - 896s 364ms/step - loss: 0.2835 - accuracy: 0.9129 - val_loss: 0.9223 - val_accuracy: 0.7700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 49/2813 [00:00<00:05, 482.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.86262\n",
            "... Validating on fold 6 ...\n",
            "... Augmenting Data ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2813/2813 [00:06<00:00, 458.40it/s]\n",
            "  1%|          | 823/78764 [00:00<00:09, 8226.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train shapes\n",
            "(78764, 600, 6) (78764,)\n",
            "... DFFT Feature Engineering ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78764/78764 [00:11<00:00, 6988.02it/s]\n",
            "100%|██████████| 312/312 [00:00<00:00, 8080.56it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "... Building Model ...\n",
            "... Training ...\n",
            "Epoch 1/200\n",
            "2462/2462 [==============================] - 936s 369ms/step - loss: 2.2364 - accuracy: 0.5095 - val_loss: 1.3186 - val_accuracy: 0.6474\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.31863, saving model to drive/MyDrive/movement_detection/kfold6/TRANSFORMER7_epoch_001_val_1.319_accuracy_0.647.h5\n",
            "Epoch 2/200\n",
            "2462/2462 [==============================] - 904s 367ms/step - loss: 1.2973 - accuracy: 0.6478 - val_loss: 0.9908 - val_accuracy: 0.7147\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.31863 to 0.99082, saving model to drive/MyDrive/movement_detection/kfold6/TRANSFORMER7_epoch_002_val_0.991_accuracy_0.715.h5\n",
            "Epoch 3/200\n",
            "2462/2462 [==============================] - 909s 369ms/step - loss: 1.0099 - accuracy: 0.7154 - val_loss: 0.8896 - val_accuracy: 0.7404\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.99082 to 0.88963, saving model to drive/MyDrive/movement_detection/kfold6/TRANSFORMER7_epoch_003_val_0.890_accuracy_0.740.h5\n",
            "Epoch 4/200\n",
            "2462/2462 [==============================] - 908s 369ms/step - loss: 0.8157 - accuracy: 0.7658 - val_loss: 0.8203 - val_accuracy: 0.7853\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.88963 to 0.82032, saving model to drive/MyDrive/movement_detection/kfold6/TRANSFORMER7_epoch_004_val_0.820_accuracy_0.785.h5\n",
            "Epoch 5/200\n",
            "1388/2462 [===============>..............] - ETA: 6:35 - loss: 0.6921 - accuracy: 0.8011"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdZvS5vfAgh5"
      },
      "source": [
        "### Make Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0daJFS7dRRaX"
      },
      "source": [
        "## Make prediction\n",
        "model1 = load_model('drive/MyDrive/movement_detection/kfold1/testNoise_epoch_004_val_0.905_accuracy_0.757.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})\n",
        "model2 = load_model('drive/MyDrive/movement_detection/kfold2/testNoise_epoch_002_val_1.010_accuracy_0.725.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder}) \n",
        "model3 = load_model('drive/MyDrive/movement_detection/kfold3/testNoise_epoch_004_val_0.957_accuracy_0.751.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})  \n",
        "model4 = load_model('drive/MyDrive/movement_detection/kfold4/testNoise_epoch_002_val_0.994_accuracy_0.709.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder}) \n",
        "model5 = load_model('drive/MyDrive/movement_detection/kfold5/testNoise_epoch_004_val_0.966_accuracy_0.748.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})\n",
        "model6 = load_model('drive/MyDrive/movement_detection/kfold6/testNoise_epoch_004_val_0.854_accuracy_0.772.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})\n",
        "model7 = load_model('drive/MyDrive/movement_detection/kfold7/testNoise_epoch_003_val_1.004_accuracy_0.731.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})\n",
        "model8 = load_model('drive/MyDrive/movement_detection/kfold8/testNoise_epoch_004_val_0.859_accuracy_0.763.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})\n",
        "model9 = load_model('drive/MyDrive/movement_detection/kfold9/testNoise_epoch_002_val_1.107_accuracy_0.734.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder}) \n",
        "model10 = load_model('drive/MyDrive/movement_detection/kfold10/testNoise_epoch_004_val_0.966_accuracy_0.763.h5',\n",
        "                     custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyBqcbLCRRXe"
      },
      "source": [
        "X_test = tf.reshape(np.array(test_features.iloc[:,2:]),[-1, 600, 6])\n",
        "X_test = np.asarray(X_test)\n",
        "\n",
        "##### feature engineering for test dataset ##### \n",
        "X_test_fourier_real = [] \n",
        "X_test_fourier_imag = [] \n",
        "for i in tqdm(range(X_test.shape[0]), position = 0, leave = True):\n",
        "    real_part = np.fft.fft(X_test[i]).real \n",
        "    imag_part = np.fft.fft(X_test[i]).imag \n",
        "    X_test_fourier_real.append(real_part) \n",
        "    X_test_fourier_imag.append(imag_part)\n",
        "    \n",
        "X_test_fourier_real = np.asarray(X_test_fourier_real) \n",
        "X_test_fourier_imag = np.asarray(X_test_fourier_imag)\n",
        "X_test = np.concatenate([X_test, X_test_fourier_real, X_test_fourier_imag], axis = 2)  \n",
        "\n",
        "print(X_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzyCe824RRVB"
      },
      "source": [
        "p1 = model1.predict(X_test) \n",
        "p2 = model2.predict(X_test)\n",
        "p3 = model3.predict(X_test) \n",
        "p4 = model4.predict(X_test) \n",
        "p5 = model5.predict(X_test) \n",
        "p6 = model6.predict(X_test) \n",
        "p7 = model7.predict(X_test) \n",
        "p8 = model8.predict(X_test)\n",
        "p9 = model9.predict(X_test)\n",
        "p10 = model10.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CjO-AEc-oqe"
      },
      "source": [
        "p_avg = (p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p10)/10.0\n",
        "\n",
        "sample_submission.iloc[:,1:] = p_avg\n",
        "\n",
        "sample_submission.to_csv(\"drive/MyDrive/movement_detection/TRANSFORMER7.csv\",index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAbF_BPT-ool"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkt7pInh-om-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXsa-hpb-okg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCILAoqJ-oiT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rK606ZP-ogf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe5l4IX6-oeV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3fBfhFIVIPB"
      },
      "source": [
        "## Define TextBook Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJpi08dVw_wv"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v): \n",
        "    matmul_qk = tf.matmul(q, k, transpose_b = True) \n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32) \n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) \n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)\n",
        "    output = tf.matmul(attention_weights, v) \n",
        "    return output, attention_weights"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p6L5gJ0fLGZ"
      },
      "source": [
        "class MultiHeadAttention(Layer): \n",
        "    def __init__(self, **kargs): \n",
        "      super(MultiHeadAttention, self).__init__()\n",
        "      self.num_heads = kargs['num_heads'] \n",
        "      self.d_model = kargs['d_model'] \n",
        "      assert self.d_model % self.num_heads == 0 \n",
        "      self.depth = self.d_model // self.num_heads \n",
        "      self.wq = Dense(kargs['d_model']) \n",
        "      self.wk = Dense(kargs['d_model']) \n",
        "      self.wv = Dense(kargs['d_model']) \n",
        "      self.dense = Dense(kargs['d_model']) \n",
        "    \n",
        "    def split_heads(self, x, batch_size): \n",
        "      x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) \n",
        "      return tf.transpose(x, perm = [0,2,1,3])  \n",
        "    \n",
        "    def call(self, v, k, q): \n",
        "      batch_size = tf.shape(q)[0] \n",
        "      q = self.wq(q) \n",
        "      k = self.wk(k) \n",
        "      v = self.wv(v) \n",
        "      q = self.split_heads(q, batch_size) \n",
        "      k = self.split_heads(k, batch_size) \n",
        "      v = self.split_heads(v, batch_size) \n",
        "      scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v) \n",
        "      scaled_attention = tf.transpose(scaled_attention, perm = [0,2,1,3]) \n",
        "      concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) \n",
        "      output = self.dense(concat_attention) \n",
        "      return output "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFr1MCeZ3oMv"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff): \n",
        "    return tf.keras.Sequential([\n",
        "      Dense(dff, activation = 'relu'), \n",
        "      Dense(d_model)\n",
        "    ])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VylhAmjV36zg"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka4kp9uZ4BzD"
      },
      "source": [
        "class EncoderLayer(Layer): \n",
        "    def __init__(self, **kargs): \n",
        "      super(EncoderLayer, self).__init__()\n",
        "      self.mha = MultiHeadAttention(**kargs) \n",
        "      self.ffn = point_wise_feed_forward_network(kargs['d_model'], kargs['dff']) \n",
        "      self.layernorm1 = LayerNormalization(epsilon = 1e-6) \n",
        "      self.layernorm2 = LayerNormalization(epsilon = 1e-6) \n",
        "      self.dropout1 = Dropout(kargs['rate']) \n",
        "      self.dropout2 = Dropout(kargs['rate']) \n",
        "    \n",
        "    def call(self, x): \n",
        "      attn_output = self.mha(x, x, x)\n",
        "      attn_output = self.dropout1(attn_output) \n",
        "      out1 = self.layernorm1(x + attn_output) \n",
        "\n",
        "      ffn_output = self.ffn(out1) \n",
        "      ffn_output = self.dropout2(ffn_output) \n",
        "      out2 = self.layernorm2(out1 + ffn_output)  \n",
        "\n",
        "      return out2 "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_15aC6JD7cW4"
      },
      "source": [
        "kargs = {'num_layers': 2,\n",
        "         'd_model': 512, \n",
        "         'num_heads': 8, \n",
        "         'dff': 2048, \n",
        "         'rate': 0.1}"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtPVEiy9AWqP"
      },
      "source": [
        "def build_model(seq_len, features, num_encoders): \n",
        "  inputs = Input(shape=(seq_len,features))   \n",
        "  bn = BatchNormalization()(inputs)    \n",
        "\n",
        "  # pass input through convolution layers instaed of word embeddings \n",
        "  conv = Conv1D(256, 5, activation = 'relu', padding = 'same')(bn) \n",
        "  bn = BatchNormalization()(conv) \n",
        "  maxpool = MaxPooling1D()(bn) \n",
        "  avgpool = AveragePooling1D()(bn) \n",
        "  pooling = Concatenate()([maxpool,avgpool])   \n",
        "  conv = Conv1D(512, 3, activation = 'relu', padding = 'same')(pooling) \n",
        "  features = BatchNormalization()(conv) \n",
        "\n",
        "  # add positional encoding \n",
        "  d_k = features.shape[2]\n",
        "  pos_encoding = positional_encoding(seq_len, d_k)    \n",
        "  features *= tf.math.sqrt(tf.cast(d_k, tf.float32)) \n",
        "  features += pos_encoding[:, :features.shape[1], :]\n",
        "  x = Dropout(0.1)(features)   \n",
        "\n",
        "  # pass through Transformer encoder layers \n",
        "  enc_layers = [EncoderLayer(**kargs) for _ in range(num_encoders)]\n",
        "  for i in range(num_encoders):  \n",
        "    x = enc_layers[i](x) \n",
        "  \n",
        "  gru_decoder = Bidirectional(GRU(128, return_sequences = False))(x) \n",
        "  dropout = Dropout(0.25)(gru_decoder) \n",
        "  dense = Dense(128, activation = 'relu')(dropout) \n",
        "  bn = BatchNormalization()(dense) \n",
        "  outputs = Dense(61, activation='softmax')(bn) \n",
        "  model = Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoDgrG_vkArG",
        "outputId": "a69494b2-e898-4858-83da-1aa2135a1d37"
      },
      "source": [
        "model = build_model(600,18,3) \n",
        "model.summary()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           [(None, 600, 18)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 600, 18)      72          input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, 600, 256)     23296       batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 600, 256)     1024        conv1d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_19 (MaxPooling1D) (None, 300, 256)     0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling1d_19 (AveragePo (None, 300, 256)     0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 300, 512)     0           max_pooling1d_19[0][0]           \n",
            "                                                                 average_pooling1d_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 300, 512)     786944      concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 300, 512)     2048        conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply_11 (TFOpLambda (None, 300, 512)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_11 (TFOpLa (None, 300, 512)     0           tf.math.multiply_11[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_82 (Dropout)            (None, 300, 512)     0           tf.__operators__.add_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_layer_33 (EncoderLayer) (None, 300, 512)     3152384     dropout_82[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder_layer_34 (EncoderLayer) (None, 300, 512)     3152384     encoder_layer_33[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "encoder_layer_35 (EncoderLayer) (None, 300, 512)     3152384     encoder_layer_34[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 256)          493056      encoder_layer_35[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_89 (Dropout)            (None, 256)          0           bidirectional_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_227 (Dense)               (None, 128)          32896       dropout_89[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 128)          512         dense_227[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_228 (Dense)               (None, 61)           7869        batch_normalization_41[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 10,804,869\n",
            "Trainable params: 10,803,041\n",
            "Non-trainable params: 1,828\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4rj1wLDjXFK"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N84-fi1rf7OP",
        "outputId": "8567d939-d334-467d-d74b-0ab7bea7f691"
      },
      "source": [
        "kfold = StratifiedKFold(n_splits = 10, random_state = 960418, shuffle = True)\n",
        "for idx,(train_idx, val_idx) in enumerate(kfold.split(X,y)):   \n",
        "    print(\"... Validating on fold {} ...\".format(idx+1)) \n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx] \n",
        "    \n",
        "    ##### augment data #####\n",
        "    print(\"... Augmenting Data ...\")\n",
        "    X_augmented = [] \n",
        "    y_augmented = [] \n",
        "    for i in tqdm(range(X_train.shape[0]), position = 0, leave = True): \n",
        "        for j in range(10): \n",
        "            shifted = np.roll(X_train[i], int(random.random() * 600)) \n",
        "            X_augmented.append(shifted) \n",
        "            y_augmented.append(y_train[i]) \n",
        "        for j in range(10): \n",
        "            noised = np.random.normal(0, 1, X_train[i].shape) + X_train[i] \n",
        "            X_augmented.append(noised) \n",
        "            y_augmented.append(y_train[i]) \n",
        "    X_augmented = np.asarray(X_augmented) \n",
        "    y_augmented = np.asarray(y_augmented)\n",
        "    X_train = np.concatenate([X_train, X_augmented]) \n",
        "    y_train = np.concatenate([y_train, y_augmented])   \n",
        "\n",
        "\n",
        "    ###### feature engineering data ##### \n",
        "    print(\"... DFFT Feature Engineering ...\")\n",
        "    X_fourier_real = [] \n",
        "    X_fourier_imag = [] \n",
        "    for i in tqdm(range(X_train.shape[0]), position = 0, leave = True):  \n",
        "        real_part = np.fft.fft(X_train[i]).real \n",
        "        imag_part = np.fft.fft(X_train[i]).imag \n",
        "        X_fourier_real.append(real_part)\n",
        "        X_fourier_imag.append(imag_part) \n",
        "    \n",
        "    X_fourier_real = np.asarray(X_fourier_real)  \n",
        "    X_fourier_imag = np.asarray(X_fourier_imag)\n",
        "    \n",
        "    \n",
        "    X_val_fourier_real = [] \n",
        "    X_val_fourier_imag = [] \n",
        "    for i in tqdm(range(X_val.shape[0]), position = 0, leave = True):\n",
        "        real_part = np.fft.fft(X_val[i]).real \n",
        "        imag_part = np.fft.fft(X_val[i]).imag \n",
        "        X_val_fourier_real.append(real_part) \n",
        "        X_val_fourier_imag.append(imag_part)\n",
        "    \n",
        "    X_val_fourier_real = np.asarray(X_val_fourier_real) \n",
        "    X_val_fourier_imag = np.asarray(X_val_fourier_imag)\n",
        "    \n",
        "    X_train = np.concatenate([X_train, X_fourier_real, X_fourier_imag], axis = 2)  \n",
        "    X_val = np.concatenate([X_val, X_val_fourier_real, X_val_fourier_imag], axis = 2)\n",
        "\n",
        "\n",
        "    ##### train model #####  \n",
        "    print(\"... Building Model ...\")\n",
        "    # we have 18 features after feature engineering \n",
        "    model = build_model(600, 18, 3) \n",
        "    print(\"... Training ...\") \n",
        "    model_path = 'drive/MyDrive/movement_detection/kfold' + str(idx+1) + '/TRANSFORMER1_epoch_{epoch:03d}_val_{val_loss:.3f}_accuracy_{val_accuracy:.3f}.h5'\n",
        "    learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', patience = 1, verbose = 1, factor = 0.8)\n",
        "    checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
        "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5) \n",
        "\n",
        "    model.fit(X_train,\n",
        "              y_train,\n",
        "              epochs = 200,\n",
        "              batch_size = 32, # hyperparameter \n",
        "              validation_data = (X_val, y_val), \n",
        "              callbacks = [learning_rate_reduction, checkpoint, early_stopping]) \n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 57/2812 [00:00<00:04, 562.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "... Validating on fold 1 ...\n",
            "... Augmenting Data ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2812/2812 [00:05<00:00, 508.08it/s]\n",
            "  2%|▏         | 1052/59052 [00:00<00:05, 10515.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "... DFFT Feature Engineering ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 59052/59052 [00:07<00:00, 8275.30it/s]\n",
            "100%|██████████| 313/313 [00:00<00:00, 10292.57it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "... Building Model ...\n",
            "... Training ...\n",
            "Epoch 1/200\n",
            "1846/1846 [==============================] - 218s 115ms/step - loss: 3.0465 - accuracy: 0.4295 - val_loss: 2.9091 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.90915, saving model to drive/MyDrive/movement_detection/kfold1/TRANSFORMER1_epoch_001_val_2.909_accuracy_0.486.h5\n",
            "Epoch 2/200\n",
            "1846/1846 [==============================] - 211s 114ms/step - loss: 2.7863 - accuracy: 0.4858 - val_loss: 2.7619 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.90915 to 2.76195, saving model to drive/MyDrive/movement_detection/kfold1/TRANSFORMER1_epoch_002_val_2.762_accuracy_0.486.h5\n",
            "Epoch 3/200\n",
            "1846/1846 [==============================] - 212s 115ms/step - loss: 2.7677 - accuracy: 0.4870 - val_loss: 2.7608 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.76195 to 2.76078, saving model to drive/MyDrive/movement_detection/kfold1/TRANSFORMER1_epoch_003_val_2.761_accuracy_0.486.h5\n",
            "Epoch 4/200\n",
            "1846/1846 [==============================] - 211s 114ms/step - loss: 2.7669 - accuracy: 0.4857 - val_loss: 2.7601 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.76078 to 2.76012, saving model to drive/MyDrive/movement_detection/kfold1/TRANSFORMER1_epoch_004_val_2.760_accuracy_0.486.h5\n",
            "Epoch 5/200\n",
            "1846/1846 [==============================] - 212s 115ms/step - loss: 2.7532 - accuracy: 0.4877 - val_loss: 2.7547 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.76012 to 2.75470, saving model to drive/MyDrive/movement_detection/kfold1/TRANSFORMER1_epoch_005_val_2.755_accuracy_0.486.h5\n",
            "Epoch 6/200\n",
            "1846/1846 [==============================] - 212s 115ms/step - loss: 2.7614 - accuracy: 0.4849 - val_loss: 2.7552 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 2.75470\n",
            "Epoch 7/200\n",
            "1846/1846 [==============================] - 212s 115ms/step - loss: 2.7443 - accuracy: 0.4896 - val_loss: 2.7554 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 2.75470\n",
            "Epoch 8/200\n",
            "1846/1846 [==============================] - 212s 115ms/step - loss: 2.7689 - accuracy: 0.4827 - val_loss: 2.7547 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.75470 to 2.75469, saving model to drive/MyDrive/movement_detection/kfold1/TRANSFORMER1_epoch_008_val_2.755_accuracy_0.486.h5\n",
            "Epoch 9/200\n",
            "1846/1846 [==============================] - 212s 115ms/step - loss: 2.7680 - accuracy: 0.4826 - val_loss: 2.7547 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 2.75469\n",
            "Epoch 10/200\n",
            "1846/1846 [==============================] - 212s 115ms/step - loss: 2.7695 - accuracy: 0.4826 - val_loss: 2.7543 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.75469 to 2.75431, saving model to drive/MyDrive/movement_detection/kfold1/TRANSFORMER1_epoch_010_val_2.754_accuracy_0.486.h5\n",
            "Epoch 11/200\n",
            "1846/1846 [==============================] - 212s 115ms/step - loss: 2.7610 - accuracy: 0.4842 - val_loss: 2.7543 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.75431 to 2.75426, saving model to drive/MyDrive/movement_detection/kfold1/TRANSFORMER1_epoch_011_val_2.754_accuracy_0.486.h5\n",
            "Epoch 12/200\n",
            "1846/1846 [==============================] - 212s 115ms/step - loss: 2.7803 - accuracy: 0.4795 - val_loss: 2.7536 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.75426 to 2.75361, saving model to drive/MyDrive/movement_detection/kfold1/TRANSFORMER1_epoch_012_val_2.754_accuracy_0.486.h5\n",
            "Epoch 13/200\n",
            "1846/1846 [==============================] - 212s 115ms/step - loss: 2.7660 - accuracy: 0.4828 - val_loss: 2.7546 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 2.75361\n",
            "Epoch 14/200\n",
            "1846/1846 [==============================] - 212s 115ms/step - loss: 2.7499 - accuracy: 0.4860 - val_loss: 2.7541 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 2.75361\n",
            "Epoch 15/200\n",
            "1846/1846 [==============================] - 211s 114ms/step - loss: 2.7664 - accuracy: 0.4826 - val_loss: 2.7542 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 2.75361\n",
            "Epoch 16/200\n",
            "1846/1846 [==============================] - 211s 115ms/step - loss: 2.7498 - accuracy: 0.4871 - val_loss: 2.7535 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.75361 to 2.75350, saving model to drive/MyDrive/movement_detection/kfold1/TRANSFORMER1_epoch_016_val_2.754_accuracy_0.486.h5\n",
            "Epoch 17/200\n",
            "1846/1846 [==============================] - 211s 114ms/step - loss: 2.7629 - accuracy: 0.4840 - val_loss: 2.7536 - val_accuracy: 0.4856\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 2.75350\n",
            "Epoch 18/200\n",
            " 497/1846 [=======>......................] - ETA: 2:34 - loss: 2.7652 - accuracy: 0.4831"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-64c26d0e634a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m               \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# hyperparameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m               \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m               callbacks = [learning_rate_reduction, checkpoint, early_stopping]) \n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX9OE0wZ3XoG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3chyis2Mw9UK"
      },
      "source": [
        "## Make Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtnhr29iw88Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVOSO9lqjQw6"
      },
      "source": [
        "model1 = load_model('drive/MyDrive/movement_detection/kfold1/testNoise_epoch_004_val_0.905_accuracy_0.757.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})\n",
        "model2 = load_model('drive/MyDrive/movement_detection/kfold2/testNoise_epoch_002_val_1.010_accuracy_0.725.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder}) \n",
        "model3 = load_model('drive/MyDrive/movement_detection/kfold3/testNoise_epoch_004_val_0.957_accuracy_0.751.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})  \n",
        "model4 = load_model('drive/MyDrive/movement_detection/kfold4/testNoise_epoch_002_val_0.994_accuracy_0.709.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder}) \n",
        "model5 = load_model('drive/MyDrive/movement_detection/kfold5/testNoise_epoch_004_val_0.966_accuracy_0.748.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})\n",
        "model6 = load_model('drive/MyDrive/movement_detection/kfold6/testNoise_epoch_004_val_0.854_accuracy_0.772.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})\n",
        "model7 = load_model('drive/MyDrive/movement_detection/kfold7/testNoise_epoch_003_val_1.004_accuracy_0.731.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})\n",
        "model8 = load_model('drive/MyDrive/movement_detection/kfold8/testNoise_epoch_004_val_0.859_accuracy_0.763.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})\n",
        "model9 = load_model('drive/MyDrive/movement_detection/kfold9/testNoise_epoch_002_val_1.107_accuracy_0.734.h5',\n",
        "                    custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder}) \n",
        "model10 = load_model('drive/MyDrive/movement_detection/kfold10/testNoise_epoch_004_val_0.966_accuracy_0.763.h5',\n",
        "                     custom_objects={'SingleAttention': SingleAttention,\n",
        "                                    'MultiAttention': MultiAttention,\n",
        "                                    'TransformerEncoder': TransformerEncoder})\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}